{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5544e97656202",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sparse Retrieval\n",
    "Implementation of sparse passage retrieval using TF-IDF and BM25. Evaluated on the MS MARCO dataset using MRR and retrieval time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733441e9ce7ced5f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:26.589751Z",
     "start_time": "2025-05-07T02:24:22.316190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.dataset_loader import DatasetLoader\n",
    "\n",
    "# Load cranfield dataset for testing functions, use MS MARCO, HotpotQA, and potentially Climate-FEVER for real evaluation\n",
    "loader = DatasetLoader(\"cranfield\")\n",
    "docs, queries, qrels = loader.get_all()\n",
    "loader.print_info()\n",
    "\n",
    "# # load multiple datasets for evaluation\n",
    "# nameset = [\"beir/msmarco/dev\", \"bier/hotpotqa/dev\", \"bier/climate-fever/dev\"]\n",
    "#\n",
    "# # Dictionary to hold datasets\n",
    "# datasets = {}\n",
    "#\n",
    "# for name in nameset:\n",
    "#     loader = DatasetLoader(name)\n",
    "#     docs, queries, qrels = loader.get_all()\n",
    "#     datasets[name] = {\n",
    "#         \"docs\": docs,\n",
    "#         \"queries\": queries,\n",
    "#         \"qrels\": qrels\n",
    "#     }\n",
    "#     loader.print_info()\n"
   ],
   "id": "2f068a2c3b484ca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: cranfield\n",
      "DOCS (1400): ('1', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .') \n",
      "\n",
      "QUERIES (225): ('1', 'what similarity laws must be obeyed when constructing aeroelastic models\\nof heated high speed aircraft .') \n",
      "\n",
      "QRELS (225): ('1', {'195': 4, '142': 4, '52': 4, '462': 4, '13': 4, '14': 4, '15': 4, '56': 3, '95': 3, '30': 3, '185': 3, '858': 3, '66': 3, '876': 3, '102': 3, '51': 3, '12': 3, '879': 3, '880': 3, '37': 3, '497': 3, '184': 2, '859': 2, '378': 2, '57': 2, '31': 2, '29': 2, '875': 2, '486': -1}) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "5a95df75887d1be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:28.422128Z",
     "start_time": "2025-05-07T02:24:28.418542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract document and query IDs + texts\n",
    "doc_ids, doc_texts = list(docs.keys()), list(docs.values())\n",
    "query_ids, query_texts = list(queries.keys()), list(queries.values())"
   ],
   "id": "9484c6615a9bcd64",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Retriever",
   "id": "3bb2584d7dbeab21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:31.006797Z",
     "start_time": "2025-05-07T02:24:30.717620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "doc_tfidf = tfidf.fit_transform(doc_texts)\n",
    "query_tfidf = tfidf.transform(query_texts)\n",
    "\n",
    "# BM25\n",
    "# Use the same preprocessor and tokenizer as TF-IDF\n",
    "tokenized_docs = [tfidf.build_tokenizer()(tfidf.build_preprocessor()(doc)) for doc in doc_texts]\n",
    "tokenized_queries = [tfidf.build_tokenizer()(tfidf.build_preprocessor()(query)) for query in query_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n"
   ],
   "id": "743822846af111ab",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "73655e7f11c89ae2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Time",
   "id": "ac12a065bcfea685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:34.279200Z",
     "start_time": "2025-05-07T02:24:33.718210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "# get similarity score between documents and queries\n",
    "tfidf_start = time.time()\n",
    "similarity_matrix = cosine_similarity(query_tfidf, doc_tfidf)  # all queries at once\n",
    "tfidf_total_time = time.time() - tfidf_start\n",
    "\n",
    "tfidf_time = tfidf_total_time / len(query_ids)\n",
    "print(f\"TF-IDF average retrieval time per query: {tfidf_time:.6f} seconds\")\n",
    "\n",
    "tfidf_scores = {}\n",
    "for i, query_id in enumerate(query_ids):\n",
    "    scores = similarity_matrix[i]\n",
    "    tfidf_scores[str(query_id)] = {\n",
    "        str(doc_ids[j]): float(score)  # convert numpy.float to native float\n",
    "        for j, score in enumerate(scores)\n",
    "        if score > 0\n",
    "    }\n",
    "\n",
    "# Score each query against the corpus\n",
    "bm25_scores = {}\n",
    "bm25_total_time = 0  # total time for all queries\n",
    "\n",
    "for query_id, query_tokens in zip(query_ids, tokenized_queries):\n",
    "    start_time = time.time()\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    bm25_total_time += time.time() - start_time\n",
    "\n",
    "    bm25_scores[query_id] = {\n",
    "        doc_ids[i]: float(score)  # convert numpy.float to native float\n",
    "        for i, score in enumerate(scores)\n",
    "        if score > 0\n",
    "    }\n",
    "\n",
    "# Average retrieval time per query\n",
    "bm25_time = bm25_total_time / len(query_ids)\n",
    "print(f\"BM25 average retrieval time per query: {bm25_time:.6f} seconds\")"
   ],
   "id": "e98054cb013b9f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF average retrieval time per query: 0.000030 seconds\n",
      "BM25 average retrieval time per query: 0.002107 seconds\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mean Reciprocal Rank (MRR)",
   "id": "cba34a67ebea546c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:38.715669Z",
     "start_time": "2025-05-07T02:24:37.956310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ranx import Run, evaluate\n",
    "\n",
    "# calculate MRR\n",
    "# Run: stores the relevance scores estimated by the model under evaluation\n",
    "tfidf_run = Run.from_dict(tfidf_scores, name=\"tfidf\")\n",
    "bm25_run = Run.from_dict(bm25_scores, name=\"bm25\")\n",
    "\n",
    "tfidf_mrr = evaluate(qrels, tfidf_run, \"mrr\", make_comparable=True)\n",
    "bm25_mrr = evaluate(qrels, bm25_run, \"mrr\", make_comparable=True)\n",
    "print(f\"TF-IDF MRR: {tfidf_mrr:.4f}\")\n",
    "print(f\"BM25 MRR: {bm25_mrr:.4f}\")"
   ],
   "id": "292cdff3442db8e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF MRR: 0.4989\n",
      "BM25 MRR: 0.4987\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T02:24:40.360405Z",
     "start_time": "2025-05-07T02:24:40.343532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice\n",
    "\n",
    "def print_topk_comparison(tfidf_scores, bm25_scores, qrels, k=5, num_queries=3):\n",
    "    \"\"\"\n",
    "    Print the top-k documents for each query from both TF-IDF and BM25 scores. Correct relevant documents will be wrapped in *stars*.\n",
    "    \"\"\"\n",
    "    print(f\"{'Query ID':<10} | {'TF-IDF Top-K':<40} | {'BM25 Top-K':<40}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for query_id in islice(tfidf_scores.keys(), num_queries):\n",
    "        tfidf_top = sorted(tfidf_scores[query_id].items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        bm25_top = sorted(bm25_scores[query_id].items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        tfidf_docs = [f\"*{doc_id}*\" if doc_id in qrels.get(query_id, {}) and qrels[query_id][doc_id] > 0 else doc_id\n",
    "                      for doc_id, _ in tfidf_top]\n",
    "        bm25_docs = [f\"*{doc_id}*\" if doc_id in qrels.get(query_id, {}) and qrels[query_id][doc_id] > 0 else doc_id\n",
    "                     for doc_id, _ in bm25_top]\n",
    "\n",
    "        print(f\"{query_id:<10} | {', '.join(tfidf_docs):<40} | {', '.join(bm25_docs):<40}\")\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "qrels_dict = qrels.to_dict()\n",
    "print_topk_comparison(tfidf_scores, bm25_scores, qrels_dict, k=5, num_queries=5)\n"
   ],
   "id": "a365354e995c5ae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID   | TF-IDF Top-K                             | BM25 Top-K                              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1          | *13*, *184*, *12*, *51*, 486             | *184*, 486, *13*, *12*, 1268            \n",
      "2          | *12*, *51*, *746*, 884, 1169             | *12*, *746*, 792, *14*, 1089            \n",
      "3          | *5*, 485, *399*, *181*, *144*            | *5*, *399*, *181*, *144*, 485           \n",
      "4          | *166*, 1275, 488, 1189, *236*            | *166*, 1189, 488, 1061, 185             \n",
      "5          | 103, 746, 1272, 1379, 410                | 103, 1032, 943, *1296*, 746             \n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
