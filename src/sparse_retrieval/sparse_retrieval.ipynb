{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5544e97656202",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sparse Retrieval\n",
    "Implementation of sparse passage retrieval using TF-IDF and BM25. Evaluated on the MS MARCO dataset using MRR and retrieval time."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T00:26:53.839423Z",
     "start_time": "2025-05-07T00:26:53.835691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import time\n",
    "from ranx import Run, Qrels, evaluate"
   ],
   "id": "22ba1b96984f43ca",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "733441e9ce7ced5f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T00:01:48.374237Z",
     "start_time": "2025-05-07T00:01:44.436444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.dataset_loader import DatasetLoader\n",
    "\n",
    "# Load cranfield dataset for testing functions, use MS MARCO, HotpotQA, and potentially Climate-FEVER for real evaluation\n",
    "loader = DatasetLoader(\"cranfield\")\n",
    "docs, queries, qrels = loader.get_all()\n",
    "loader.print_info()\n",
    "\n",
    "# # load multiple datasets for evaluation\n",
    "# nameset = [\"beir/msmarco/dev\", \"bier/hotpotqa/dev\", \"bier/climate-fever/dev\"]\n",
    "#\n",
    "# # Dictionary to hold datasets\n",
    "# datasets = {}\n",
    "#\n",
    "# for name in nameset:\n",
    "#     loader = DatasetLoader(name)\n",
    "#     docs, queries, qrels = loader.get_all()\n",
    "#     datasets[name] = {\n",
    "#         \"docs\": docs,\n",
    "#         \"queries\": queries,\n",
    "#         \"qrels\": qrels\n",
    "#     }\n",
    "#     loader.print_info()\n"
   ],
   "id": "2f068a2c3b484ca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: cranfield\n",
      "DOCS (1400): ('1', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .') \n",
      "\n",
      "QUERIES (225): ('1', 'what similarity laws must be obeyed when constructing aeroelastic models\\nof heated high speed aircraft .') \n",
      "\n",
      "QRELS (225): ('1', {'195': 4, '142': 4, '52': 4, '462': 4, '13': 4, '14': 4, '15': 4, '56': 3, '95': 3, '30': 3, '185': 3, '858': 3, '66': 3, '876': 3, '102': 3, '51': 3, '12': 3, '879': 3, '880': 3, '37': 3, '497': 3, '184': 2, '859': 2, '378': 2, '57': 2, '31': 2, '29': 2, '875': 2, '486': -1}) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "5a95df75887d1be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T00:13:23.543507Z",
     "start_time": "2025-05-07T00:13:23.530499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract document and query IDs + texts\n",
    "doc_ids, doc_texts = list(docs.keys()), list(docs.values())\n",
    "query_ids, query_texts = list(queries.keys()), list(queries.values())"
   ],
   "id": "9484c6615a9bcd64",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Retriever",
   "id": "3bb2584d7dbeab21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T00:21:55.603491Z",
     "start_time": "2025-05-07T00:21:55.381749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "tfidf_matrix = tfidf.fit_transform(doc_texts)\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "#print(similarity_matrix[0][:10]) # similarity scores for the first document\n",
    "\n",
    "# # BM25\n",
    "# Use the same preprocessor and tokenizer as TF-IDF\n",
    "tokenized_docs = [tfidf.build_tokenizer()(tfidf.build_preprocessor()(doc)) for doc in doc_texts]\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "print(bm25.get_scores(tokenized_docs[0])) # BM25 scores for the first document\n"
   ],
   "id": "743822846af111ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[455.6084743  128.73215535  58.3061003  ... 119.11721915 101.34736483\n",
      " 115.11643683]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "73655e7f11c89ae2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T00:27:02.524449Z",
     "start_time": "2025-05-07T00:27:00.742336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "tfidf_results = {}\n",
    "for i in range(len(doc_texts)):\n",
    "    query_id = f\"{i}\"\n",
    "    ranked_docs = {}\n",
    "    # Get similarity scores and sort in descending order\n",
    "    similarity_scores = similarity_matrix[i]\n",
    "    sorted_indices = np.argsort(similarity_scores)[::-1]\n",
    "    for rank, doc_index in enumerate(sorted_indices):\n",
    "        if doc_index != i:  # Exclude the query document itself\n",
    "            ranked_docs[str(doc_index)] = similarity_scores[doc_index]\n",
    "    tfidf_results[query_id] = ranked_docs\n",
    "\n",
    "print(\"TF-IDF Results (ranx format):\")\n",
    "print(tfidf_results)\n",
    "\n",
    "# TO-DO: get BM25 results"
   ],
   "id": "e98054cb013b9f7d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Time",
   "id": "ac12a065bcfea685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "\n",
    "# get retrieval time for tfidf, do the same for BM25\n",
    "k = 10\n",
    "start_time = time.time()\n",
    "# do retrieval here\n",
    "retrieval_time = (time.time() - start_time) / len(query_ids)\n",
    "print(f\"Retrieval time per query: {retrieval_time:.4f} seconds\")"
   ],
   "id": "a3fe836dacb4398a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mean Reciprocal Rank (MRR)",
   "id": "cba34a67ebea546c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from ranx import Run, evaluate\n",
    "\n",
    "# calculate MRR\n",
    "\n",
    "# Run: stores the relevance scores estimated by the model under evaluation\n",
    "# map results for each query_id -> { doc_id: score }\n",
    "run = {\n",
    "    # TO-DO: edit this to use BM25 results\n",
    "    query_ids[i]: {\n",
    "        doc_ids[knn[i][j]]: -float(distances[i][j]) for j in range(k) # use negative distance as ranx interprets higher score = higher rank\n",
    "    }\n",
    "    for i in range(len(query_ids))\n",
    "}\n",
    "run_rx = Run(run)\n",
    "\n",
    "# measure MRR\n",
    "mrr = evaluate(qrels, run_rx, \"mrr\", make_comparable=True)\n",
    "print(f\"MRR: {mrr:.4f}\")\n"
   ],
   "id": "292cdff3442db8e3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
