{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dense Retrieval\n",
    "Implementation of dense passage retrieval using DistilBERT and FAISS. Evaluated on the MS MARCO dataset using MRR and retrieval time.\n"
   ],
   "id": "337d670bef39b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "c4a3a069c56effd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ir_datasets import load\n",
    "from ranx import Qrels\n",
    "from itertools import islice\n",
    "\n",
    "# choose dataset\n",
    "# dataset_name = \"beir/msmarco/dev\"\n",
    "dataset_name = \"beir/hotpotqa/dev\"\n",
    "# dataset_name = \"beir/climate-fever\"\n",
    "\n",
    "# choose subset of docs\n",
    "# subset = 1_000_000\n",
    "# subset = 2_000_000\n",
    "subset = 3_000_000\n",
    "\n",
    "# load dataset\n",
    "dataset = load(dataset_name)\n",
    "qrels = Qrels.from_ir_datasets(dataset_name)\n",
    "\n",
    "# load a subset of docs and queries\n",
    "docs = {d.doc_id: d.text for d in islice(dataset.docs_iter(), subset)} # load subset\n",
    "queries = {q.query_id: q.text for q in islice(dataset.queries_iter(), 500)} # load first 500\n",
    "\n",
    "# # uncomment to load full dataset\n",
    "# docs = {d.doc_id: d.text for d in dataset.docs_iter()}\n",
    "# queries = {q.query_id: q.text for q in dataset.queries_iter()}\n",
    "\n",
    "print(f'DOCS ({len(docs)}): {list(docs.items())[0]} \\n')\n",
    "print(f'QUERIES ({len(queries)}): {list(queries.items())[0]} \\n')\n",
    "print(f'QRELS ({len(qrels)}): {list(qrels.to_dict().items())[0]} \\n')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "27689d18561fefeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract document and query IDs + texts\n",
    "doc_ids, doc_texts = list(docs.keys()), list(docs.values())\n",
    "query_ids, query_texts = list(queries.keys()), list(queries.values())\n",
    "\n",
    "# clean qrels - remove doc_ids that dont exist in subset\n",
    "qrels_dict = qrels.to_dict()\n",
    "\n",
    "filtered_qrels_dict = {\n",
    "    qid: {\n",
    "        did: rel for did, rel in dids.items() if did in doc_ids\n",
    "    }\n",
    "    for qid, dids in qrels_dict.items()\n",
    "    if qid in query_ids and any(did in doc_ids for did in dids)\n",
    "}\n",
    "\n",
    "qrels = Qrels.from_dict(filtered_qrels_dict)"
   ],
   "id": "81e00d907f865a81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Document Embedding",
   "id": "3be1476ef6e7ca04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.quantization import quantize_embeddings\n",
    "import torch\n",
    "\n",
    "# load encoder model pretrained on MS MARCO\n",
    "model_name = \"msmarco-MiniLM-L6-cos-v5\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 32 if device == \"cuda\" else 16\n",
    "model.eval() # put in eval mode to speed up inference\n",
    "\n",
    "# encode documents and queries\n",
    "document_embeddings = model.encode(doc_texts,\n",
    "                                    batch_size=batch_size,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    device=device,\n",
    "                                    show_progress_bar=True)\n",
    "query_embeddings = model.encode(query_texts,\n",
    "                                batch_size=batch_size,\n",
    "                                convert_to_tensor=True,\n",
    "                                device=device,\n",
    "                                show_progress_bar=True)\n",
    "\n",
    "# convert to numpy array for FAISS\n",
    "doc_embs = document_embeddings.cpu().numpy().astype(\"float32\")\n",
    "query_embs = query_embeddings.cpu().numpy().astype(\"float32\")"
   ],
   "id": "39aac114369a4200",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build FAISS Index",
   "id": "4eddcf3b18d99939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import faiss\n",
    "\n",
    "#normalize embeddings with faiss\n",
    "faiss.normalize_L2(doc_embs)\n",
    "faiss.normalize_L2(query_embs)\n",
    "\n",
    "dim = doc_embs.shape[1]\n",
    "print(dim)\n",
    "\n",
    "# build index, use cosine similarity\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(doc_embs)\n",
    "print(index.ntotal)\n"
   ],
   "id": "6427809a7fb07cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "d67c58a609d8787a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Time",
   "id": "a3d42d5ca82d091c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# do KNN Search and return retrieval time\n",
    "k = 10\n",
    "start_time = time.time()\n",
    "distances, indices = index.search(query_embs, k)\n",
    "retrieval_time = (time.time() - start_time) / len(query_ids)\n",
    "print(f\"Retrieval time per query: {retrieval_time:.6f} seconds\")"
   ],
   "id": "f2e4b89791c2f550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mean Reciprocal Rank (MRR)",
   "id": "2b73ff489de39052"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ranx import Run, evaluate\n",
    "\n",
    "# calculate MRR\n",
    "\n",
    "# Run: stores the relevance scores estimated by the model under evaluation\n",
    "# map results for each query_id -> { doc_id: score }\n",
    "run = {\n",
    "    query_ids[i]: {\n",
    "        doc_ids[indices[i][j]]: float(distances[i][j]) for j in range(k)\n",
    "    }\n",
    "    for i in range(len(query_ids))\n",
    "}\n",
    "\n",
    "# convert to Run object\n",
    "run_rx = Run(run)\n",
    "\n",
    "# measure MRR\n",
    "mrr = evaluate(qrels=qrels, run=run_rx, metrics=\"mrr\", make_comparable=True)\n",
    "print(f\"MRR: {mrr:.8f}\")\n"
   ],
   "id": "35ff9ccd078436fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "name = \"hotpot\" if \"hotpot\" in dataset_name else (\"msmarco\" if \"msmarco\" in dataset_name else \"climate-fever\")\n",
    "\n",
    "# # save run to google drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# run_rx.save(f\"/gdrive/MyDrive/{name}_{subset}_dense.json\")\n",
    "\n",
    "# save locally\n",
    "run_rx.save(f\"data/{name}/{subset}_dense.json\")\n",
    "\n",
    "# re-print results\n",
    "print(f\"dataset: {dataset_name}, num of docs: {len(docs)}\")\n",
    "print(f\"Dense retrieval time per query: {retrieval_time:.6f} seconds\")\n",
    "print(f\"MRR: {mrr:.8f}\")"
   ],
   "id": "f3c9170bed5ce5be",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
