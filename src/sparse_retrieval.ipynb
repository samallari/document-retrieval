{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5544e97656202",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sparse Retrieval\n",
    "Implementation of sparse passage retrieval using TF-IDF and BM25. Evaluated on the MS MARCO dataset using MRR and retrieval time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733441e9ce7ced5f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:04.599668Z",
     "start_time": "2025-05-07T06:04:00.559389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataset_loader import DatasetLoader\n",
    "\n",
    "# Load cranfield dataset for testing functions, use MS MARCO, HotpotQA, and potentially Climate-FEVER for real evaluation\n",
    "loader = DatasetLoader(\"cranfield\")\n",
    "docs, queries, qrels = loader.get_all()\n",
    "loader.print_info()\n",
    "\n",
    "# # load multiple datasets for evaluation\n",
    "# nameset = [\"beir/msmarco/dev\", \"bier/hotpotqa/dev\", \"bier/climate-fever/dev\"]\n",
    "#\n",
    "# # Dictionary to hold datasets\n",
    "# datasets = {}\n",
    "#\n",
    "# for name in nameset:\n",
    "#     loader = DatasetLoader(name)\n",
    "#     docs, queries, qrels = loader.get_all()\n",
    "#     datasets[name] = {\n",
    "#         \"docs\": docs,\n",
    "#         \"queries\": queries,\n",
    "#         \"qrels\": qrels\n",
    "#     }\n",
    "#     loader.print_info()\n"
   ],
   "id": "2f068a2c3b484ca8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET: cranfield\n",
      "DOCS (1400): ('1', 'experimental investigation of the aerodynamics of a\\nwing in a slipstream .\\n  an experimental study of a wing in a propeller slipstream was\\nmade in order to determine the spanwise distribution of the lift\\nincrease due to slipstream at different angles of attack of the wing\\nand at different free stream to slipstream velocity ratios .  the\\nresults were intended in part as an evaluation basis for different\\ntheoretical treatments of this problem .\\n  the comparative span loading curves, together with\\nsupporting evidence, showed that a substantial part of the lift increment\\nproduced by the slipstream was due to a /destalling/ or\\nboundary-layer-control effect .  the integrated remaining lift\\nincrement, after subtracting this destalling lift, was found to agree\\nwell with a potential flow theory .\\n  an empirical evaluation of the destalling effects was made for\\nthe specific configuration of the experiment .') \n",
      "\n",
      "QUERIES (225): ('1', 'what similarity laws must be obeyed when constructing aeroelastic models\\nof heated high speed aircraft .') \n",
      "\n",
      "QRELS (225): ('1', {'195': 4, '142': 4, '52': 4, '462': 4, '13': 4, '14': 4, '15': 4, '56': 3, '95': 3, '30': 3, '185': 3, '858': 3, '66': 3, '876': 3, '102': 3, '51': 3, '12': 3, '879': 3, '880': 3, '37': 3, '497': 3, '184': 2, '859': 2, '378': 2, '57': 2, '31': 2, '29': 2, '875': 2, '486': -1}) \n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "5a95df75887d1be8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:04.722608Z",
     "start_time": "2025-05-07T06:04:04.719876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract document and query IDs + texts\n",
    "doc_ids, doc_texts = list(docs.keys()), list(docs.values())\n",
    "query_ids, query_texts = list(queries.keys()), list(queries.values())"
   ],
   "id": "9484c6615a9bcd64",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Retriever",
   "id": "3bb2584d7dbeab21"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:05.024089Z",
     "start_time": "2025-05-07T06:04:04.734123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\")\n",
    "doc_tfidf = tfidf.fit_transform(doc_texts)\n",
    "query_tfidf = tfidf.transform(query_texts)\n",
    "\n",
    "# convert to array for FAISS\n",
    "doc_tfidf_arr = doc_tfidf.toarray().astype('float32')\n",
    "query_tfidf_arr = query_tfidf.toarray().astype('float32')\n",
    "\n",
    "# normalize\n",
    "faiss.normalize_L2(doc_tfidf_arr)\n",
    "faiss.normalize_L2(query_tfidf_arr)\n",
    "\n",
    "# build index\n",
    "dim = doc_tfidf_arr.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # Inner product (cosine similarity)\n",
    "index.add(doc_tfidf_arr)  # add documents to index\n",
    "# print(index.ntotal)\n",
    "\n",
    "# BM25\n",
    "# Use the same preprocessor and tokenizer as TF-IDF\n",
    "tokenized_docs = [tfidf.build_tokenizer()(tfidf.build_preprocessor()(doc)) for doc in doc_texts]\n",
    "tokenized_queries = [tfidf.build_tokenizer()(tfidf.build_preprocessor()(query)) for query in query_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n"
   ],
   "id": "743822846af111ab",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "73655e7f11c89ae2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Time",
   "id": "ac12a065bcfea685"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:05.580128Z",
     "start_time": "2025-05-07T06:04:05.040179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "from heapq import nlargest\n",
    "\n",
    "k = 10\n",
    "\n",
    "start_time = time.time()\n",
    "distances, indices = index.search(query_tfidf_arr, k)  # search for top k documents\n",
    "retrieval_time = (time.time() - start_time) / len(query_ids)\n",
    "print(f\"TF-IDF Retrieval time per query: {retrieval_time:.6f} seconds\")\n",
    "\n",
    "# store results in a dictionary\n",
    "tfidf_scores = {\n",
    "    query_ids[i]: {\n",
    "        doc_ids[indices[i][j]]: float(distances[i][j]) for j in range(k)\n",
    "    }\n",
    "    for i in range(len(query_ids))\n",
    "}\n",
    "\n",
    "# Score each query against the corpus\n",
    "bm25_scores = {}\n",
    "bm25_total_time = 0  # total time for all queries\n",
    "\n",
    "for query_id, query_tokens in zip(query_ids, tokenized_queries):\n",
    "    start_time = time.time()\n",
    "    scores = bm25.get_scores(query_tokens) # get scores for all documents\n",
    "    bm25_total_time += time.time() - start_time\n",
    "\n",
    "    # Get top-k indices sorted by score\n",
    "    top_k = nlargest(k, enumerate(scores), key=lambda x: x[1])\n",
    "\n",
    "    bm25_scores[query_id] = {\n",
    "        doc_ids[i]: float(score)\n",
    "        for i, score in top_k\n",
    "    }\n",
    "\n",
    "# Average retrieval time per query\n",
    "bm25_time = bm25_total_time / len(query_ids)\n",
    "print(f\"BM25 Retrieval time per query: {bm25_time:.6f} seconds\")"
   ],
   "id": "e98054cb013b9f7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Retrieval time per query: 0.000048 seconds\n",
      "BM25 Retrieval time per query: 0.002192 seconds\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mean Reciprocal Rank (MRR)",
   "id": "cba34a67ebea546c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:06.445115Z",
     "start_time": "2025-05-07T06:04:05.596155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ranx import Run, evaluate\n",
    "\n",
    "# calculate MRR\n",
    "# Run: stores the relevance scores estimated by the model under evaluation\n",
    "tfidf_run = Run.from_dict(tfidf_scores, name=\"tfidf\")\n",
    "bm25_run = Run.from_dict(bm25_scores, name=\"bm25\")\n",
    "\n",
    "# save results to file\n",
    "tfidf_run.save(\"tfidf_run.json\")\n",
    "bm25_run.save(\"bm25_run.json\")\n",
    "\n",
    "tfidf_mrr = evaluate(qrels, tfidf_run, \"mrr@10\", make_comparable=True)\n",
    "bm25_mrr = evaluate(qrels, bm25_run, \"mrr@10\", make_comparable=True)\n",
    "print(f\"TF-IDF MRR: {tfidf_mrr:.4f}\")\n",
    "print(f\"BM25 MRR: {bm25_mrr:.4f}\")"
   ],
   "id": "292cdff3442db8e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF MRR: 0.4920\n",
      "BM25 MRR: 0.4928\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T06:04:06.482821Z",
     "start_time": "2025-05-07T06:04:06.472656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import islice\n",
    "\n",
    "def print_topk_comparison(tfidf_scores, bm25_scores, qrels, k=10, num_queries=5):\n",
    "    \"\"\"\n",
    "    Print the top-k documents for each query from both TF-IDF and BM25 scores. Correct relevant documents will be wrapped in *stars*.\n",
    "    \"\"\"\n",
    "    print(f\"{'Query ID':<10} | {'TF-IDF Top-K':<40} | {'BM25 Top-K':<40}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for query_id in islice(tfidf_scores.keys(), num_queries):\n",
    "        tfidf_top = sorted(tfidf_scores[query_id].items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "        bm25_top = sorted(bm25_scores[query_id].items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        tfidf_docs = [f\"*{doc_id}*\" if doc_id in qrels.get(query_id, {}) and qrels[query_id][doc_id] > 0 else doc_id\n",
    "                      for doc_id, _ in tfidf_top]\n",
    "        bm25_docs = [f\"*{doc_id}*\" if doc_id in qrels.get(query_id, {}) and qrels[query_id][doc_id] > 0 else doc_id\n",
    "                     for doc_id, _ in bm25_top]\n",
    "\n",
    "        print(f\"{query_id:<10} | {', '.join(tfidf_docs):<40} | {', '.join(bm25_docs):<40}\")\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "qrels_dict = qrels.to_dict()\n",
    "print_topk_comparison(tfidf_scores, bm25_scores, qrels_dict, k=5, num_queries=5)\n"
   ],
   "id": "a365354e995c5ae4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID   | TF-IDF Top-K                             | BM25 Top-K                              \n",
      "----------------------------------------------------------------------------------------------------\n",
      "1          | *13*, *184*, *12*, *51*, 486             | *184*, 486, *13*, *12*, 1268            \n",
      "2          | *12*, *51*, *746*, 884, 1169             | *12*, *746*, 792, *14*, 1089            \n",
      "3          | *5*, 485, *399*, *181*, *144*            | *5*, *399*, *181*, *144*, 485           \n",
      "4          | *166*, 1275, 488, 1189, *236*            | *166*, 1189, 488, 1061, 185             \n",
      "5          | 103, 746, 1272, 1379, 410                | 103, 1032, 943, *1296*, 746             \n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
