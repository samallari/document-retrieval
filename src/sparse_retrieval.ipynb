{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c5544e97656202",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sparse Retrieval\n",
    "Implementation of sparse passage retrieval using TF-IDF and BM25. Evaluated using MRR and retrieval time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733441e9ce7ced5f",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ir_datasets import load\n",
    "from ranx import Qrels\n",
    "from itertools import islice\n",
    "\n",
    "# choose dataset\n",
    "# dataset_name = \"beir/msmarco/dev\"\n",
    "# dataset_name = \"beir/hotpotqa/dev\"\n",
    "dataset_name = \"beir/climate-fever\"\n",
    "\n",
    "# choose subset of docs\n",
    "subset = 1_000_000\n",
    "# subset = 2_000_000\n",
    "# subset = 3_000_000\n",
    "\n",
    "# ==== do not change below ====\n",
    "\n",
    "# load dataset\n",
    "dataset = load(dataset_name)\n",
    "qrels = Qrels.from_ir_datasets(dataset_name)\n",
    "\n",
    "# load a subset of docs and queries\n",
    "docs = {d.doc_id: d.text for d in islice(dataset.docs_iter(), subset)} # load subset\n",
    "queries = {q.query_id: q.text for q in islice(dataset.queries_iter(), 500)} # load first 500\n",
    "\n",
    "# # uncomment to load full dataset\n",
    "# docs = {d.doc_id: d.text for d in dataset.docs_iter()}\n",
    "# queries = {q.query_id: q.text for q in dataset.queries_iter()}\n",
    "\n",
    "print(f'DOCS ({len(docs)}): {list(docs.items())[0]} \\n')\n",
    "print(f'QUERIES ({len(queries)}): {list(queries.items())[0]} \\n')\n",
    "print(f'QRELS ({len(qrels)}): {list(qrels.to_dict().items())[0]} \\n')"
   ],
   "id": "2f068a2c3b484ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "5a95df75887d1be8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract document and query IDs + texts\n",
    "doc_ids, doc_texts = list(docs.keys()), list(docs.values())\n",
    "query_ids, query_texts = list(queries.keys()), list(queries.values())\n",
    "\n",
    "# clean qrels - remove doc_ids that dont exist in subset\n",
    "qrels_dict = qrels.to_dict()\n",
    "\n",
    "filtered_qrels_dict = {\n",
    "    qid: {\n",
    "        did: rel for did, rel in dids.items() if did in doc_ids\n",
    "    }\n",
    "    for qid, dids in qrels_dict.items()\n",
    "    if qid in query_ids and any(did in doc_ids for did in dids)\n",
    "}\n",
    "\n",
    "qrels = Qrels.from_dict(filtered_qrels_dict)"
   ],
   "id": "9484c6615a9bcd64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Retriever",
   "id": "3bb2584d7dbeab21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import time\n",
    "\n",
    "# TF-IDF\n",
    "print(\"Building TF-IDF\")\n",
    "start_tfidf = time.time()\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=True, stop_words=\"english\", max_features=50_000)\n",
    "doc_tfidf = tfidf.fit_transform(doc_texts)\n",
    "query_tfidf = tfidf.transform(query_texts)\n",
    "\n",
    "finish_tfidf = time.time() - start_tfidf\n",
    "print(f\"Finished TF-IDF: {finish_tfidf:.4f}s\")\n",
    "\n",
    "# BM25\n",
    "\n",
    "print(\"Building BM25\")\n",
    "start_bm25 = time.time()\n",
    "\n",
    "# Use the same preprocessor and tokenizer as TF-IDF\n",
    "preprocessor = tfidf.build_preprocessor()\n",
    "tokenizer = tfidf.build_tokenizer()\n",
    "\n",
    "tokenized_docs = [tokenizer(preprocessor(doc)) for doc in doc_texts]\n",
    "tokenized_queries = [tokenizer(preprocessor(query)) for query in query_texts]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "\n",
    "finish_bm25 = time.time() - start_bm25\n",
    "print(f\"Finished BM25: {finish_bm25:.4f}s\")\n"
   ],
   "id": "743822846af111ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73655e7f11c89ae2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Retrieval Time",
   "id": "ac12a065bcfea685"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "k = 10 # top-k docs\n",
    "\n",
    "# ====== TF-IDF ======\n",
    "batch_size = 100\n",
    "tfidf_scores = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in tqdm(range(0, query_tfidf.shape[0], batch_size), desc=\"TF-IDF - Retrieving\"):\n",
    "    batch_queries = query_tfidf[i:i + batch_size]\n",
    "    sims = cosine_similarity(batch_queries, doc_tfidf)\n",
    "\n",
    "    # build dict of scores\n",
    "    for bi, sim_row in enumerate(sims):\n",
    "        qid = query_ids[i + bi]\n",
    "\n",
    "        top_k_idx = np.argpartition(sim_row, -k)[-k:]\n",
    "        top_k_idx = top_k_idx[np.argsort(sim_row[top_k_idx])[::-1]]\n",
    "\n",
    "        tfidf_scores[qid] = {\n",
    "            doc_ids[j]: float(sim_row[j]) for j in top_k_idx\n",
    "        }\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "retrieval_time = total_time / len(query_ids)\n",
    "print(f\"TF-IDF retrieval time per query: {retrieval_time:.6f} seconds\")\n",
    "\n",
    "# ====== BM25 ======\n",
    "# Score each query against the corpus\n",
    "bm25_scores = {}\n",
    "bm25_total_time = 0  # total time for all queries\n",
    "\n",
    "for query_id, query_tokens in tqdm(zip(query_ids, tokenized_queries), total=len(query_ids), desc=\"BM25 - Retrieving\"):\n",
    "    start_time = time.time()\n",
    "    # get scores for all documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    bm25_total_time += time.time() - start_time\n",
    "\n",
    "    # get top-k scores\n",
    "    top_k_idx = np.argpartition(scores, -k)[-k:]\n",
    "    top_k_idx = top_k_idx[np.argsort(scores[top_k_idx])[::-1]]\n",
    "\n",
    "    # build dict of scores\n",
    "    bm25_scores[query_id] = {\n",
    "        doc_ids[i]: float(scores[i])\n",
    "        for i in top_k_idx\n",
    "    }\n",
    "\n",
    "\n",
    "# Average retrieval time per query\n",
    "bm25_time = bm25_total_time / len(query_ids)\n",
    "print(f\"BM25 Retrieval time per query: {bm25_time:.6f} seconds\")"
   ],
   "id": "e98054cb013b9f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Mean Reciprocal Rank (MRR)",
   "id": "cba34a67ebea546c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ranx import Run, evaluate\n",
    "\n",
    "# calculate MRR\n",
    "# Run: stores the relevance scores estimated by the model under evaluation\n",
    "tfidf_run = Run.from_dict(tfidf_scores, name=\"tfidf\")\n",
    "bm25_run = Run.from_dict(bm25_scores, name=\"bm25\")\n",
    "\n",
    "tfidf_mrr = evaluate(qrels=qrels, run=tfidf_run, metrics=\"mrr\", make_comparable=True) # make_comparable removes query ids that are not in both qrels and run\n",
    "bm25_mrr = evaluate(qrels=qrels, run=bm25_run, metrics=\"mrr\", make_comparable=True)\n",
    "print(f\"TF-IDF MRR: {tfidf_mrr:.6f}\")\n",
    "print(f\"BM25 MRR: {bm25_mrr:.6f}\")"
   ],
   "id": "292cdff3442db8e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save results to file\n",
    "name = \"hotpot\" if \"hotpot\" in dataset_name else (\"msmarco\" if \"msmarco\" in dataset_name else \"climate-fever\")\n",
    "\n",
    "tfidf_run.save(f\"data/{name}/{subset}_tfidf_run.json\")\n",
    "bm25_run.save(f\"data/{name}/{subset}_bm25_run.json\")\n",
    "\n",
    "# re-print results at the end for convenience\n",
    "print(f\"dataset: {name}, num of docs: {len(docs)}\")\n",
    "print(f\"TF-IDF retrieval time per query: {retrieval_time:.6f} seconds\")\n",
    "print(f\"BM25 Retrieval time per query: {bm25_time:.6f} seconds\")\n",
    "print(f\"TF-IDF MRR: {tfidf_mrr:.6f}\")\n",
    "print(f\"BM25 MRR: {bm25_mrr:.6f}\")"
   ],
   "id": "1b8e92a4046e33ef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
